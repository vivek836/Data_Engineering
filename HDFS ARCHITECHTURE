https://www.edureka.co/blog/apache-hadoop-hdfs-architecture/

ETL Pipeline (Extract - Transform - Load)
Data Pipelines can be distributed over multiple servers. Below is one common ETL Pipeline, but it can be varied depending on different Sources, and their usages.

ETL

The Data can come from any Source: it can either be from Stored Files or Streaming Data from IoT Devices, System Logs, User Actvity on an App, etc.

E (Extraction)
Extraction takes care of extracting the data from different Sources, stored or streaming data.

T (Transformation)
Transformation takes care of processing the data, identifying and removing anomalies, decides schema according to the Data.

L (Loading)
Loading takes care of storing the data to the Storage either Data Mart, Data Warehouse, etc. which will later be utilized by multiple teams as per their requirement.

Further References to study about Data Engineering and Skills required:

https://realpython.com/python-data-engineer/
https://github.com/datastacktv/data-engineer-roadmap
